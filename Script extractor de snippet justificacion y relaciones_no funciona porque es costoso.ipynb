{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPpqyjffUjw4toJSY9gq/aW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":256},"id":"Ys_AhcRNCB4D","executionInfo":{"status":"error","timestamp":1769460229937,"user_tz":-60,"elapsed":1635,"user":{"displayName":"ayelen cura","userId":"07173400498953091069"}},"outputId":"b2f4d821-6e46-4c33-ec76-2983ee014336"},"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'OPENAI_API_KEY'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-202169483.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mOUTPUT_GRAPH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"graph_enriched.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# ================= CARGA =================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'OPENAI_API_KEY'"]}],"source":["from openai import OpenAI\n","import json\n","import os\n","from tqdm import tqdm\n","\n","# ================= CONFIGURACI√ìN =================\n","\n","MODEL = \"gpt-4.1\"          # recomendado para extracci√≥n de evidencia\n","CHUNK_SIZE = 30           # 20‚Äì50 es seguro\n","INPUT_GRAPH = \"graph_input.json\"\n","PROMPT_FILE = \"prompt.txt\"\n","OUTPUT_GRAPH = \"graph_enriched.json\"\n","\n","client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n","\n","# ================= CARGA =================\n","\n","with open(INPUT_GRAPH, \"r\", encoding=\"utf-8\") as f:\n","    graph = json.load(f)\n","\n","with open(PROMPT_FILE, \"r\", encoding=\"utf-8\") as f:\n","    system_prompt = f.read()\n","\n","nodes = graph.get(\"nodes\", [])\n","relationships = graph.get(\"relationships\", [])\n","\n","# √çndice r√°pido de relaciones por nodo\n","rels_by_node = {}\n","for r in relationships:\n","    rels_by_node.setdefault(r[\"source\"], []).append(r)\n","    rels_by_node.setdefault(r[\"target\"], []).append(r)\n","\n","# ================= UTILIDADES =================\n","\n","def chunk_list(lst, size):\n","    for i in range(0, len(lst), size):\n","        yield lst[i:i+size]\n","\n","enriched_nodes = []\n","enriched_relationships = []\n","processed_rel_ids = set()\n","\n","# ================= PROCESAMIENTO =================\n","\n","for node_chunk in tqdm(list(chunk_list(nodes, CHUNK_SIZE))):\n","\n","    node_ids = {n[\"id\"] for n in node_chunk}\n","\n","    # relaciones asociadas a estos nodos\n","    rel_chunk = []\n","    for nid in node_ids:\n","        for r in rels_by_node.get(nid, []):\n","            # evitar duplicarlas muchas veces\n","            rid = (r[\"source\"], r[\"target\"], r[\"type\"])\n","            if rid not in processed_rel_ids:\n","                rel_chunk.append(r)\n","                processed_rel_ids.add(rid)\n","\n","    partial_graph = {\n","        \"nodes\": node_chunk,\n","        \"relationships\": rel_chunk\n","    }\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_prompt},\n","        {\n","            \"role\": \"user\",\n","            \"content\": json.dumps(partial_graph, ensure_ascii=False)\n","        }\n","    ]\n","\n","    response = client.chat.completions.create(\n","        model=MODEL,\n","        messages=messages,\n","        temperature=0.0,\n","        max_tokens=8000\n","    )\n","\n","    raw_output = response.choices[0].message.content\n","\n","    try:\n","        enriched_partial = json.loads(raw_output)\n","    except json.JSONDecodeError:\n","        # guardamos el output crudo para depurar\n","        with open(\"error_raw_output.txt\", \"w\", encoding=\"utf-8\") as f:\n","            f.write(raw_output)\n","        raise RuntimeError(\"‚ùå El modelo devolvi√≥ JSON inv√°lido. Ver error_raw_output.txt\")\n","\n","    enriched_nodes.extend(enriched_partial.get(\"nodes\", []))\n","    enriched_relationships.extend(enriched_partial.get(\"relationships\", []))\n","\n","# ================= GUARDADO FINAL =================\n","\n","final_graph = {\n","    \"nodes\": enriched_nodes,\n","    \"relationships\": enriched_relationships\n","}\n","\n","with open(OUTPUT_GRAPH, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(final_graph, f, indent=2, ensure_ascii=False)\n","\n","print(\"‚úÖ Grafo enriquecido guardado en:\", OUTPUT_GRAPH)\n"]},{"cell_type":"markdown","metadata":{"id":"158dbade"},"source":["1.  **Access Secrets Manager**: On the left-hand side panel of Colab, click on the \"üîë Secrets\" icon.\n","2.  **Add a new secret**: Click on \"Add new secret\".\n","3.  **Name the secret**: In the \"Name\" field, type `OPENAI_API_KEY` (ensure it matches exactly, including capitalization).\n","4.  **Enter your API key**: In the \"Value\" field, paste your actual OpenAI API key.\n","5.  **Save the secret**: Make sure \"Notebook access\" is toggled ON for this secret.\n","\n","Once you've added the secret, you can access it in your notebook using `userdata.get()` from the `google.colab` library. I will now generate a code cell to demonstrate this and fix the previous error."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":347},"id":"86ee45ca","executionInfo":{"status":"error","timestamp":1769460378984,"user_tz":-60,"elapsed":636,"user":{"displayName":"ayelen cura","userId":"07173400498953091069"}},"outputId":"e5380be1-f145-4fd0-9fb2-41193a4020c3"},"source":["import os\n","from google.colab import userdata\n","\n","# Retrieve the API key from Colab's Secrets Manager\n","os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n","\n","# You can then initialize your OpenAI client as before\n","# from openai import OpenAI\n","# client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n","\n","print(\"OPENAI_API_KEY has been loaded from Colab Secrets.\")"],"execution_count":2,"outputs":[{"output_type":"error","ename":"SecretNotFoundError","evalue":"Secret OPENAI_API_KEY does not exist.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-534482860.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Retrieve the API key from Colab's Secrets Manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# You can then initialize your OpenAI client as before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mSecretNotFoundError\u001b[0m: Secret OPENAI_API_KEY does not exist."]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":462},"id":"f16f842b","executionInfo":{"status":"error","timestamp":1769461610060,"user_tz":-60,"elapsed":62679,"user":{"displayName":"ayelen cura","userId":"07173400498953091069"}},"outputId":"c950c9d4-6acd-484a-8881-ac4f11c814b0"},"source":["from openai import OpenAI\n","import json\n","import os\n","from tqdm import tqdm\n","from google.colab import userdata\n","\n","# ================= CONFIGURACI√ìN =================\n","\n","MODEL = \"gpt-4.1\"          # recomendado para extracci√≥n de evidencia\n","CHUNK_SIZE = 30           # 20‚Äì50 es seguro\n","INPUT_GRAPH = \"/content/final_graph_enriched_metadata_clean_260126_juani_sin snippet, justif y conf.json\"\n","PROMPT_FILE = \"/content/prompt agregar snippet, justif y confianza 26.01.txt\"\n","OUTPUT_GRAPH = \"graph_enriched.json\"\n","\n","# Retrieve the API key from Colab's Secrets Manager\n","os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n","client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n","\n","# ================= CARGA =================\n","\n","with open(INPUT_GRAPH, \"r\", encoding=\"utf-8\") as f:\n","    graph = json.load(f)\n","\n","with open(PROMPT_FILE, \"r\", encoding=\"utf-8\") as f:\n","    system_prompt = f.read()\n","\n","nodes = graph.get(\"nodes\", [])\n","relationships = graph.get(\"relationships\", [])\n","\n","# √çndice r√°pido de relaciones por nodo\n","rels_by_node = {}\n","for r in relationships:\n","    rels_by_node.setdefault(r[\"source\"], []).append(r)\n","    rels_by_node.setdefault(r[\"target\"], []).append(r)\n","\n","# ================= UTILIDADES =================\n","\n","def chunk_list(lst, size):\n","    for i in range(0, len(lst), size):\n","        yield lst[i:i+size]\n","\n","enriched_nodes = []\n","enriched_relationships = []\n","processed_rel_ids = set()\n","\n","# ================= PROCESAMIENTO =================\n","\n","for node_chunk in tqdm(list(chunk_list(nodes, CHUNK_SIZE))):\n","\n","    node_ids = {n[\"id\"] for n in node_chunk}\n","\n","    # relaciones asociadas a estos nodos\n","    rel_chunk = []\n","    for nid in node_ids:\n","        for r in rels_by_node.get(nid, []):\n","            # evitar duplicarlas muchas veces\n","            rid = (r[\"source\"], r[\"target\"], r[\"type\"])\n","            if rid not in processed_rel_ids:\n","                rel_chunk.append(r)\n","                processed_rel_ids.add(rid)\n","\n","    partial_graph = {\n","        \"nodes\": node_chunk,\n","        \"relationships\": rel_chunk\n","    }\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_prompt},\n","        {\n","            \"role\": \"user\",\n","            \"content\": json.dumps(partial_graph, ensure_ascii=False)\n","        }\n","    ]\n","\n","    response = client.chat.completions.create(\n","        model=MODEL,\n","        messages=messages,\n","        temperature=0.0,\n","        max_tokens=8000\n","    )\n","\n","    raw_output = response.choices[0].message.content\n","\n","    try:\n","        enriched_partial = json.loads(raw_output)\n","    except json.JSONDecodeError:\n","        # guardamos el output crudo para depurar\n","        with open(\"error_raw_output.txt\", \"w\", encoding=\"utf-8\") as f:\n","            f.write(raw_output)\n","        raise RuntimeError(\"‚ùå El modelo devolvi√≥ JSON inv√°lido. Ver error_raw_output.txt\")\n","\n","    enriched_nodes.extend(enriched_partial.get(\"nodes\", []))\n","    enriched_relationships.extend(enriched_partial.get(\"relationships\", []))\n","\n","# ================= GUARDADO FINAL =================\n","\n","final_graph = {\n","    \"nodes\": enriched_nodes,\n","    \"relationships\": enriched_relationships\n","}\n","\n","with open(OUTPUT_GRAPH, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(final_graph, f, indent=2, ensure_ascii=False)\n","\n","print(\"‚úÖ Grafo enriquecido guardado en:\", OUTPUT_GRAPH)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/5 [01:00<?, ?it/s]\n"]},{"output_type":"error","ename":"RateLimitError","evalue":"Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1836250627.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m     ]\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1190\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1191\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1193\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"75a18195","executionInfo":{"status":"ok","timestamp":1769461697741,"user_tz":-60,"elapsed":4925,"user":{"displayName":"ayelen cura","userId":"07173400498953091069"}},"outputId":"31ee865e-163f-4c50-963a-c34b65bf1fef"},"source":["pip install tenacity"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tenacity in /usr/local/lib/python3.12/dist-packages (9.1.2)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":555},"id":"f4ca40a3","executionInfo":{"status":"error","timestamp":1769462178238,"user_tz":-60,"elapsed":56453,"user":{"displayName":"ayelen cura","userId":"07173400498953091069"}},"outputId":"8f3ad64d-d603-4878-8713-3d2a91169fba"},"source":["import openai\n","from openai import OpenAI\n","import json\n","import os\n","from tqdm import tqdm\n","from google.colab import userdata\n","from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n","\n","# ================= CONFIGURACI√ìN =================\n","\n","MODEL = \"gpt-4.1\"          # recomendado para extracci√≥n de evidencia\n","CHUNK_SIZE = 30           # 20‚Äì50 es seguro\n","INPUT_GRAPH = \"/content/final_graph_enriched_metadata_clean_260126_juani_sin snippet, justif y conf.json\"\n","PROMPT_FILE = \"/content/prompt agregar snippet, justif y confianza 26.01.txt\"\n","OUTPUT_GRAPH = \"graph_enriched.json\"\n","\n","# Retrieve the API key from Colab's Secrets Manager\n","os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n","client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n","\n","# ================= CARGA =================\n","\n","with open(INPUT_GRAPH, \"r\", encoding=\"utf-8\") as f:\n","    graph = json.load(f)\n","\n","with open(PROMPT_FILE, \"r\", encoding=\"utf-8\") as f:\n","    system_prompt = f.read()\n","\n","nodes = graph.get(\"nodes\", [])\n","relationships = graph.get(\"relationships\", [])\n","\n","# √çndice r√°pido de relaciones por nodo\n","rels_by_node = {}\n","for r in relationships:\n","    rels_by_node.setdefault(r[\"source\"], []).append(r)\n","    rels_by_node.setdefault(r[\"target\"], []).append(r)\n","\n","# ================= UTILIDADES =================\n","\n","def chunk_list(lst, size):\n","    for i in range(0, len(lst), size):\n","        yield lst[i:i+size]\n","\n","enriched_nodes = []\n","enriched_relationships = []\n","processed_rel_ids = set()\n","\n","# Define the retry decorator\n","@retry(\n","    wait=wait_exponential(multiplier=1, min=4, max=60),\n","    stop=stop_after_attempt(6),\n","    retry=retry_if_exception_type(openai.APITimeoutError) | retry_if_exception_type(openai.APIConnectionError) | retry_if_exception_type(openai.RateLimitError)\n",")\n","def create_chat_completion_with_retry(model, messages, temperature, max_tokens):\n","    return client.chat.completions.create(\n","        model=model,\n","        messages=messages,\n","        temperature=temperature,\n","        max_tokens=max_tokens\n","    )\n","\n","# ================= PROCESAMIENTO =================\n","\n","for node_chunk in tqdm(list(chunk_list(nodes, CHUNK_SIZE))):\n","\n","    node_ids = {n[\"id\"] for n in node_chunk}\n","\n","    # relaciones asociadas a estos nodos\n","    rel_chunk = []\n","    for nid in node_ids:\n","        for r in rels_by_node.get(nid, []):\n","            # evitar duplicarlas muchas veces\n","            rid = (r[\"source\"], r[\"target\"], r[\"type\"])\n","            if rid not in processed_rel_ids:\n","                rel_chunk.append(r)\n","                processed_rel_ids.add(rid)\n","\n","    partial_graph = {\n","        \"nodes\": node_chunk,\n","        \"relationships\": rel_chunk\n","    }\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_prompt},\n","        {\n","            \"role\": \"user\",\n","            \"content\": json.dumps(partial_graph, ensure_ascii=False)\n","        }\n","    ]\n","\n","    try:\n","        response = create_chat_completion_with_retry(\n","            model=MODEL,\n","            messages=messages,\n","            temperature=0.0,\n","            max_tokens=8000\n","        )\n","    except Exception as e:\n","        print(f\"‚ùå Failed after multiple retries: {e}\")\n","        raise # Re-raise the exception if all retries fail\n","\n","    raw_output = response.choices[0].message.content\n","\n","    try:\n","        enriched_partial = json.loads(raw_output)\n","    except json.JSONDecodeError:\n","        # guardamos el output crudo para depurar\n","        with open(\"error_raw_output.txt\", \"w\", encoding=\"utf-8\") as f:\n","            f.write(raw_output)\n","        raise RuntimeError(\"‚ùå El modelo devolvi√≥ JSON inv√°lido. Ver error_raw_output.txt\")\n","\n","    enriched_nodes.extend(enriched_partial.get(\"nodes\", []))\n","    enriched_relationships.extend(enriched_partial.get(\"relationships\", []))\n","\n","# ================= GUARDADO FINAL =================\n","\n","final_graph = {\n","    \"nodes\": enriched_nodes,\n","    \"relationships\": enriched_relationships\n","}\n","\n","with open(OUTPUT_GRAPH, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(final_graph, f, indent=2, ensure_ascii=False)\n","\n","print(\"‚úÖ Grafo enriquecido guardado en:\", OUTPUT_GRAPH)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/5 [03:10<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["‚ùå Failed after multiple retries: RetryError[<Future at 0x7a37dc5cf650 state=finished raised RateLimitError>]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"error","ename":"RetryError","evalue":"RetryError[<Future at 0x7a37dc5cf650 state=finished raised RateLimitError>]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2543837391.py\u001b[0m in \u001b[0;36mcreate_chat_completion_with_retry\u001b[0;34m(model, messages, temperature, max_tokens)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_chat_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     return client.chat.completions.create(\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1193\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2543837391.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         response = create_chat_completion_with_retry(\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mwrapped_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRetryError\u001b[0m: RetryError[<Future at 0x7a37dc5cf650 state=finished raised RateLimitError>]"]}]}]}